{"status":"Succeeded","log":["JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:null->Initialized","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Initialized->Started","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Started->Running pre train steps","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Running pre train steps->Started","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Started->Process started","Model management server:http://192.168.65.2:8080/","Model:k0b4qd0TNN","Version:1","Training ID:","Downloading files for training","Python 3.5.2","*   Trying 192.168.65.2...","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current","                                 Dload  Upload   Total   Spent    Left  Speed","","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 192.168.65.2 (192.168.65.2) port 8080 (#0)","> GET //model-management/CUSTOM/model/k0b4qd0TNN/resource.zip HTTP/1.1","> Host: 192.168.65.2:8080","> User-Agent: curl/7.47.0","> Accept: */*","> ","< HTTP/1.1 200 ","< Content-Type: application/octet-stream","< Content-Length: 153030","< Date: Thu, 01 Mar 2018 21:52:00 GMT","< ","{ [4262 bytes data]","","100  149k  100  149k    0     0  4894k      0 --:--:-- --:--:-- --:--:-- 5534k","* Connection #0 to host 192.168.65.2 left intact","*   Trying 192.168.65.2...","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current","                                 Dload  Upload   Total   Spent    Left  Speed","","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 192.168.65.2 (192.168.65.2) port 8080 (#0)","> GET //model-management/CUSTOM/model/k0b4qd0TNN HTTP/1.1","> Host: 192.168.65.2:8080","> User-Agent: curl/7.47.0","> Accept: */*","> ","< HTTP/1.1 200 ","< Content-Type: application/json;charset=UTF-8","< Transfer-Encoding: chunked","< Date: Thu, 01 Mar 2018 21:52:00 GMT","< ","{ [189 bytes data]","","100   183    0   183    0     0  23797      0 --:--:-- --:--:-- --:--:-- 26142","* Connection #0 to host 192.168.65.2 left intact","Unpacking","Archive:  input/model.zip","   creating: model/","  inflating: model/model.iml         ","   creating: model/database/","  inflating: model/database/db_pandas_to_sql.pyc  ","  inflating: model/database/__init__.py  ","  inflating: model/database/__init__.pyc  ","  inflating: model/database/db_pandas_to_sql.py  ","  inflating: model/database/db_write_line_by_line.py  ","   creating: model/pipeline/","  inflating: model/pipeline/tensor_logistic_regression_classifier.py  ","  inflating: model/pipeline/pipes_np.py  ","  inflating: model/pipeline/keras_classifier.pyc  ","  inflating: model/pipeline/lime.py  ","  inflating: model/pipeline/io.py    ","  inflating: model/pipeline/__init__.py  ","   creating: model/pipeline/__pycache__/","  inflating: model/pipeline/__pycache__/keras_classifier.cpython-36.pyc  ","  inflating: model/pipeline/__pycache__/pipes_np.cpython-36.pyc  ","  inflating: model/pipeline/__pycache__/pipe.cpython-36.pyc  ","  inflating: model/pipeline/__pycache__/pipeline.cpython-36.pyc  ","  inflating: model/pipeline/__pycache__/io.cpython-36.pyc  ","  inflating: model/pipeline/__pycache__/__init__.cpython-36.pyc  ","  inflating: model/pipeline/io.pyc   ","  inflating: model/pipeline/README.md  ","  inflating: model/pipeline/pipeline.pyc  ","  inflating: model/pipeline/pipeline.py  ","  inflating: model/pipeline/__init__.pyc  ","  inflating: model/pipeline/pipes_df.py  ","  inflating: model/pipeline/keras_classifier.py  ","  inflating: model/pipeline/explanatory_pipeline.py  ","  inflating: model/pipeline/pipe.py  ","  inflating: model/pipeline/logistic_regression_classifier.py  ","  inflating: model/pipeline/convolutional_pipeline.py  ","  inflating: model/pipeline/local_explainer.py  ","  inflating: model/pipeline/pipe.pyc  ","   creating: model/docker/","   creating: model/docker/tfserving/","  inflating: model/docker/tfserving/Dockerfile  ","   creating: model/docker/trainer/","  inflating: model/docker/trainer/Dockerfile  ","  inflating: model/.DS_Store         ","   creating: __MACOSX/","   creating: __MACOSX/model/","  inflating: __MACOSX/model/._.DS_Store  ","  inflating: model/requirements.txt  ","   creating: model/test/","   creating: model/test/variables/","   creating: model/end_to_end/","  inflating: model/end_to_end/train_pipeline.py  ","  inflating: model/end_to_end/use_pipeline.py  "," extracting: model/end_to_end/__init__.py  ","  inflating: model/end_to_end/README.md  ","  inflating: model/end_to_end/define_pipeline.py  ","  inflating: model/end_to_end/explain_pipeline.py  ","  inflating: model/credentials.py    ","   creating: model/input/","  inflating: model/compare_losses.ipynb  ","   creating: model/tests/","  inflating: model/tests/auxiliar.py  ","  inflating: model/tests/test_conv_pipeline.py  ","  inflating: model/tests/test_logReg.py  "," extracting: model/tests/__init__.py  ","  inflating: model/tests/test_end_to_end.py  ","   creating: model/tests/test_iterators/","  inflating: model/tests/test_iterators/test_pickle.py  "," extracting: model/tests/test_iterators/__init__.py  ","  inflating: model/tests/test_iterators/test_csv.py  ","   creating: model/tests/test_pipeline/","  inflating: model/tests/test_pipeline/auxiliar.py  "," extracting: model/tests/test_pipeline/__init__.py  ","  inflating: model/tests/test_pipeline/test_io.py  ","  inflating: model/tests/test_pipeline/test_pipes_np.py  ","  inflating: model/tests/test_pipeline/test_kerasclassifier.py  ","  inflating: model/tests/test_pipeline/test_explanation.py  ","  inflating: model/tests/test_pipeline/test_batch.py  ","  inflating: model/tests/test_pipeline/test_pipeline.py  ","  inflating: model/tests/test_pipeline/test_pipes_df.py  ","  inflating: model/tests/test_tensor_logreg_pipeline.py  ","  inflating: model/tests/test_models.py  ","  inflating: model/tests/test_loss.py  ","   creating: model/tests/test_generators/"," extracting: model/tests/test_generators/__init__.py  ","  inflating: model/tests/test_generators/test_e_fraud.py  ","  inflating: model/generate_bust_out.py  ","   creating: model/models/","  inflating: model/models/lstm_bust_out_synthetic.py  ","  inflating: model/models/__init__.py  ","  inflating: model/models/dense_bust_out_synthetic.py  ","  inflating: model/models/README.md  ","   creating: model/__pycache__/","  inflating: model/__pycache__/credentials.cpython-36.pyc  ","  inflating: model/credentials.pyc   ","  inflating: model/README.md         ","  inflating: model/loss.py           ","  inflating: model/setup.py          ","  inflating: model/populate_server_db.py  ","   creating: model/examples/"," extracting: model/examples/__init__.py  ","  inflating: model/examples/keras_fit_all_data.py  ","  inflating: model/examples/kaggle_loaded.py  ","  inflating: model/examples/keras_pickle_input_source.py  ","  inflating: model/examples/keras_multiple_inputs.py  ","  inflating: model/examples/keras_fit_generator.py  ","   creating: model/scripts/","  inflating: model/scripts/tfserving-entry.sh  ","  inflating: model/scripts/build-trainer-image.sh  ","  inflating: model/scripts/trainer-entry.sh  ","  inflating: model/scripts/build-tfserving-image.sh  ","  inflating: model/scripts/train.py  ","   creating: model/generators/","  inflating: model/generators/e_fraud.py  "," extracting: model/generators/__init__.py  ","  inflating: model/generators/README.md  ","   creating: model/generators/bust-out/","  inflating: model/generators/bust-out/gencc.py  ","  inflating: model/generators/bust-out/bust_out.py  ","  inflating: model/generators/bust-out/generate_other_tables.py  ","  inflating: model/generators/bust-out/transaction_values.py  ","   creating: model/generators/random/","  inflating: model/generators/random/synthetic_anomalies.py  ","  inflating: model/generators/random/__init__.py  ","  inflating: model/generators/random/synth_cc  ","  inflating: model/generators/random/README.md  ","  inflating: model/generators/random/synth_template  ","   creating: model/data/"," extracting: model/data/.gitkeep     ","  inflating: model/data/__init__.py  ","   creating: model/data/__pycache__/","  inflating: model/data/__pycache__/__init__.cpython-36.pyc  ","   creating: model/iterators/","  inflating: model/iterators/dummy_iterator.py  ","  inflating: model/iterators/__init__.py  ","   creating: model/iterators/__pycache__/","  inflating: model/iterators/__pycache__/sqlalchemy.cpython-36.pyc  ","  inflating: model/iterators/__pycache__/mysql.cpython-36.pyc  ","  inflating: model/iterators/__pycache__/dummy_iterator.cpython-36.pyc  ","  inflating: model/iterators/__pycache__/__init__.cpython-36.pyc  ","  inflating: model/iterators/__pycache__/teradata.cpython-36.pyc  ","  inflating: model/iterators/README.md  ","  inflating: model/iterators/sqlalchemy.py  ","  inflating: model/iterators/csv.py  ","  inflating: model/iterators/pickle.py  ","  inflating: model/iterators/teradata.py  ","  inflating: model/iterators/mysql.py  ","Install requirements","Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 1))","Requirement already satisfied: tensorflow in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 2))","Requirement already satisfied: pandas in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 3))","Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 4))","Requirement already satisfied: sqlalchemy_utils in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 5))","Collecting pymysql (from -r ./model/requirements.txt (line 6))","  Downloading PyMySQL-0.8.0-py2.py3-none-any.whl (83kB)","Collecting cloudpickle (from -r ./model/requirements.txt (line 7))","  Downloading cloudpickle-0.5.2-py2.py3-none-any.whl","Requirement already satisfied: sklearn in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 8))","Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from -r ./model/requirements.txt (line 9))","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras->-r ./model/requirements.txt (line 1))","Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras->-r ./model/requirements.txt (line 1))","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras->-r ./model/requirements.txt (line 1))","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras->-r ./model/requirements.txt (line 1))","Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /usr/local/lib/python3.5/dist-packages (from tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas->-r ./model/requirements.txt (line 3))","Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.5/dist-packages (from pandas->-r ./model/requirements.txt (line 3))","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (from sklearn->-r ./model/requirements.txt (line 8))","Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf>=3.3.0->tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow->-r ./model/requirements.txt (line 2))","Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.5/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow->-r ./model/requirements.txt (line 2))","Installing collected packages: pymysql, cloudpickle","Successfully installed cloudpickle-0.5.2 pymysql-0.8.0","Training model","Traceback (most recent call last):","  File \"model/end_to_end/train_pipeline.py\", line 17, in <module>","    _main(\"output/untrained/1\", \"output/trained/1\")","  File \"model/end_to_end/train_pipeline.py\", line 6, in _main","    pipeline = load_untrained(directory_in)","  File \"/notebooks/model/pipeline/io.py\", line 102, in load_untrained","    with open(directory + \"/pipeline.pkl\", 'rb') as f:","FileNotFoundError: [Errno 2] No such file or directory: 'output/untrained/1/pipeline.pkl'","{\"status\":200,\"payload\":{\"modelName\":\"k0b4qd0TNN\",\"modelType\":\"CUSTOM\",\"entryPoint\":\"model/end_to_end/train_pipeline.py\",\"performanceJson\":\"performance.json\",\"outputFolder\":\"output\"}}Unable to read output folder:output","","Execution finished","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Process started->Succeeded","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Succeeded->Start training","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Start training->Started","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Started->Process started","docker: Error response from daemon: Conflict. The container name \"/custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20\" is already in use by container \"ef2ca80d5e3ed9b8fefb8649dafa69ec406d070697f01f33288b79bf6cc031f4\". You have to remove (or rename) that container to be able to reuse that name.","See 'docker run --help'.","JOB:custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20(TRAINING) status changed:Process started->Succeeded"],"id":"custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20","metadata":{"MODEL_NAME":"k0b4qd0TNN","MODEL_VERSION":"1","API_SERVER":"http://192.168.65.2:8080/","TRAINING_ID":"custom-k0b4qd0tnn-cpu-8be447b3-eddf9f20","TOKEN":"b623d656-3af9-4b65-abc2-112871633a6a","token":"b623d656-3af9-4b65-abc2-112871633a6a"}}